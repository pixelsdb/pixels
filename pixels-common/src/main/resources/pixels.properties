###### pixels-daemon settings ######
# pixels.var.dir is where the lock files are created
pixels.var.dir=/home/pixels/opt/pixels/var/
# metadata database connection properties
metadata.db.driver=com.mysql.jdbc.Driver
metadata.db.user=pixels
metadata.db.password=password
metadata.db.url=jdbc:mysql://localhost:3306/pixels_metadata?useUnicode=true&characterEncoding=UTF-8&zeroDateTimeBehavior=convertToNull
# metadata server host and port
metadata.server.port=18888
metadata.server.host=localhost
# transaction server host and port
trans.server.port=18889
trans.server.host=localhost
# query scheduling server for pixels-turbo
query.schedule.server.port=18893
query.schedule.server.host=localhost
# worker coordinate server for pixels-turbo
worker.coordinate.server.port=18894
worker.coordinate.server.host=localhost
# etcd connection properties
# multiple host can be listed as etcd.hosts=host0,host1,host2
etcd.hosts=localhost
etcd.port=2379
# metrics server properties
metrics.server.enabled=false
metrics.bytesms.interval=51200
metrics.reader.json.dir=/dev/shm/
metrics.node.text.dir=/home/pixels/opt/node_exporter/text/
metrics.reader.collect.prob=0.1

###### pixels-sink settings ######
# Presto/Trino connection properties
presto.pixels.jdbc.url=jdbc:trino://localhost:8080/pixels/tpch
presto.orc.jdbc.url=jdbc:trino://localhost:8080/hive/tpch
presto.user=pixels
presto.password=null
presto.ssl=false
presto.query.url=http://localhost:8080/v1/query

###### pixels-cache settings ######
cache.location=/mnt/ramfs/pixels.cache
cache.size=102400000
index.location=/mnt/ramfs/pixels.index
index.size=102400000
cache.storage.scheme=hdfs
cache.schema=pixels
cache.table=test_105
# lease ttl must be larger than heartbeat period
lease.ttl.seconds=20
# heartbeat period must be larger than 0
heartbeat.period.seconds=10
# set to false if storage.scheme is S3
enable.absolute.balancer=false
cache.enabled=false
cache.read.direct=false

###### storage engine settings ######

### pixels reader, writer, and compactor settings ###
# the pixels stride (number of values in a pixel) for pixels writer
pixel.stride=10000
# the row group size in bytes for pixels writer, should not exceed 2GB
row.group.size=268435456
# the alignment of the start offset of a column chunk in the file, it is for SIMD and its unit is byte
column.chunk.alignment=32
# the alignment of the start offset of the isnull bitmap in a column chunk,
# it is for the compatibility of DuckDB and its unit is byte.
# for DuckDB, it is only effective when column.chunk.alignment also meets the alignment of the isNull bitmap
isnull.bitmap.alignment=8
# whether column chunks are encoded in pixels writer
column.chunk.encoding=true
# the little-endian is used on the column chunks in pixels writer
column.chunk.little.endian=true
# the block size for block-wise storage systems such as HDFS
block.size=2147483648
# the number of replications of each block for block-wise storage systems such as HDFS
block.replication=1
# for block-wise storage systems, whether padding the leftover space in current block
block.padding=true
# the number of bytes to be compressed as a block using heavy compression algorithms
compression.block.size=1048576
# for pixels compactor how many row groups are compacted into one file
compact.factor=32
# row batch size for pixels record reader, default value is 10000
row.batch.size=10000

### file storage and I/O ###
# the scheme of the storage systems that are enabled, e.g., hdfs,file,s3,gcs,minio,redis
enabled.storage.schemes=s3,minio,file,hdfs
# which scheduler to use for read requests, valid values: noop, sortmerge, ratelimited
read.request.scheduler=sortmerge
read.request.merge.gap=0
# rate limits only work for s3+ratelimited
read.request.rate.limit.rps=16000
read.request.rate.limit.mbps=1200
read.request.enable.retry=true
read.request.max.retry.num=3
# the interval in milliseconds of retry queue checks
read.request.retry.interval.ms=1000
# the dir containing core-site.xml and hdfs-site.xml
hdfs.config.dir=/opt/hadoop-2.7.3/etc/hadoop/
s3.enable.async=true
s3.use.async.client=true
s3.connection.timeout.sec=3600
s3.connection.acquisition.timeout.sec=3600
s3.client.service.threads=40
s3.max.request.concurrency=1000
s3.max.pending.requests=100000
gcs.enable.async=true
localfs.block.size=4096
localfs.enable.direct.io=false
# if mmap is enabled, direct io will be ignored
localfs.enable.mmap=false
localfs.enable.async.io=true
localfs.async.lib=iouring
localfs.reader.threads=40
minio.region=eu-central-2
minio.endpoint=http://localhost:9000
minio.access.key=minio-access-key-dummy
minio.secret.key=minio-secret-key-dummy
redis.endpoint=localhost:6379
redis.access.key=redis-user-dummy
redis.secret.key=redis-password-dummy

###### query engine settings ######

### dynamic spitting ###
# split size will be set to this fixed value if it is positive
fixed.split.size=-1
# true to enable just-in-time splitting in ordered path
multi.split.for.ordered=true
# the size in bytes of a table scan split, 16MB by default
split.size.mb=64
# the number of rows in a table scan split, 2560000 by default, <= 0 for unlimited
split.size.rows=5120000
# the type of split size index to be used, can be cost_based or inverted
# before using cost_based, ensure data statistics are collected
splits.index.type=inverted
projection.read.enabled=false

### Presto/Trino connectors ###
record.cursor.enabled=false

### pixels-turbo - query scheduling ###
scaling.enabled=false
scaling.machine.service=ec2
scaling.mpp.queue.capacity=3
scaling.cf.queue.capacity=5
cloud.watch.metrics.namespace=Pixels
cloud.watch.metrics.dimension.name=cluster
cloud.watch.metrics.dimension.value=01
query.concurrency.metrics.name=query-concurrency
query.concurrency.report.period.sec=5

### pixels-turbo - query planning ###
join.large.side.completion.ratio=0.1
# the maximum size in megabytes of a broadcast table
join.broadcast.threshold.mb=256
# the maximum number of rows in a broadcast table
join.broadcast.threshold.rows=20480000
# the maximum (average) size of a partition in partitioned join
join.partition.size.mb=512
# the maximum (average) number of rows in a partition in partitioned join
join.partition.size.rows=20480000
# the maximum number of rows in a partition in aggregation
aggr.partition.size.rows=1280000

### pixels-turbo - query execution ###
executor.input.storage.scheme=s3
executor.intermediate.storage.scheme=s3
executor.intermediate.folder=/pixels-turbo/intermediate/
executor.output.storage.scheme=s3
executor.output.folder=/pixels-turbo/output/
executor.stage.completion.ratio=0.6
executor.selectivity.enabled=true
# the number of threads used in each worker
executor.intra.worker.parallelism=8
# which cloud function service to use, can be lambda (AWS Lambda) or vhive (vHive)
executor.function.service=lambda
# which method will be used for data exchanging, batch or stream
executor.exchange.method=batch
executor.ordered.layout.enabled=false
executor.compact.layout.enabled=true
# the names of the cloud function workers
scan.worker.name=ScanWorker
partition.worker.name=PartitionWorker
broadcast.join.worker.name=BroadcastJoinWorker
broadcast.chain.join.worker.name=BroadcastChainJoinWorker
partitioned.join.worker.name=PartitionedJoinWorker
partitioned.chain.join.worker.name=PartitionedChainJoinWorker
aggregation.worker.name=AggregationWorker
# parameter for vhive client
vhive.hostname=localhost
vhive.port=50051

###### experimental settings ######
# the rate of free memory in jvm
experimental.gc.threshold=0.3
