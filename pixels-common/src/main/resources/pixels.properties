###### pixels-daemon settings ######
# pixels.var.dir is where the lock files are created
pixels.var.dir=/home/pixels/opt/pixels/var/
# metadata database connection properties
metadata.db.driver=com.mysql.cj.jdbc.Driver
metadata.db.user=pixels
metadata.db.password=password
metadata.db.url=jdbc:mysql://localhost:3306/pixels_metadata?useUnicode=true&characterEncoding=UTF-8&zeroDateTimeBehavior=convertToNull
# metadata server host and port
metadata.server.port=18888
metadata.server.host=localhost
# transaction server host and port
trans.server.port=18889
trans.server.host=localhost
# retina server host and port
retina.server.port=18890
retina.server.host=localhost
# query scheduling server for pixels-turbo
query.schedule.server.port=18893
query.schedule.server.host=localhost
# worker coordinate server for pixels-turbo
worker.coordinate.server.port=18894
worker.coordinate.server.host=localhost
# etcd connection properties
# multiple host can be listed as etcd.hosts=host0,host1,host2
etcd.hosts=localhost
etcd.port=2379
# metrics server properties
metrics.server.enabled=false
metrics.bytesms.interval=51200
metrics.reader.json.dir=/dev/shm/
metrics.node.text.dir=/home/pixels/opt/node_exporter/text/
metrics.reader.collect.prob=0.1

###### pixels-sink settings ######
# Presto/Trino connection properties
presto.jdbc.url=jdbc:trino://localhost:8080/pixels
presto.user=pixels
presto.password=null
presto.ssl=false
presto.query.url=http://localhost:8080/v1/query

###### pixels-cache settings ######
# the base location of the cache zone files, the file name of each cache zone adds a postfix to the base location
cache.base.location=/mnt/ramfs/pixels.cache
# the user available size of the whole cache, including all zones, in bytes
cache.size=68719476736
# the location of the index files, the name of each index file adds a postfix to the base location
index.base.location=/mnt/ramfs/pixels.index
# the user available size of the whole index, including the global index and the zone indexes, in bytes
index.size=1073741824
# the number of zones in the cache
cache.zone.num=3
# the number of swap zones in the cache
cache.zone.swap.num=1
# the scheme of the storage system to be cached
cache.storage.scheme=hdfs
# set to true if cache.storage.scheme is a locality sensitive storage such as hdfs
cache.absolute.balancer.enabled=false
# set to true to enable pixels-cache
cache.enabled=false
# set to true to read cache without memory copy
cache.read.direct=false

###### pixels-heartbeat settings ######
# heartbeat lease ttl must be larger than heartbeat period
heartbeat.lease.ttl.seconds=20
# heartbeat period must be larger than 0
heartbeat.period.seconds=10

###### storage engine settings ######

### pixels reader, writer, and compactor settings ###
# the pixels stride (number of values in a pixel) for pixels writer, set it to multiple of 8 for better performance
pixel.stride=10000
# the row group size in bytes for pixels writer, should not exceed 2GB
row.group.size=268435456
# the alignment of the start offset of a column chunk in the file, it is for SIMD and its unit is byte
column.chunk.alignment=32
# the alignment of the start offset of the isnull bitmap in a column chunk,
# it is for the compatibility of DuckDB and its unit is byte.
# for DuckDB, it is only effective when column.chunk.alignment also meets the alignment of the isNull bitmap
isnull.bitmap.alignment=8
# whether column chunks are encoded in pixels writer
column.chunk.encoding=true
# whether little-endian is used on the column chunks in pixels writer
column.chunk.little.endian=true
# the block size for block-wise storage systems such as HDFS
block.size=2147483648
# the number of replications of each block for block-wise storage systems such as HDFS
block.replication=1
# for block-wise storage systems, whether padding the leftover space in current block
block.padding=true
# the number of bytes to be compressed as a block using heavy compression algorithms
compression.block.size=1048576
# for pixels compactor how many row groups are compacted into one file
compact.factor=32
# row batch size for pixels record reader, default value is 10000
row.batch.size=10000

### file storage and I/O ###
# the scheme of the storage systems that are enabled, e.g., hdfs,file,s3,gcs,minio,redis
enabled.storage.schemes=s3,minio,file,httpstream
# which scheduler to use for read requests, valid values: noop, sortmerge, ratelimited
read.request.scheduler=sortmerge
read.request.merge.gap=0
# rate limits only work for s3+ratelimited
read.request.rate.limit.rps=16000
read.request.rate.limit.mbps=1200
read.request.enable.retry=true
read.request.max.retry.num=3
# the interval in milliseconds of retry queue checks
read.request.retry.interval.ms=1000
# the dir containing core-site.xml and hdfs-site.xml
hdfs.config.dir=/opt/hadoop-2.7.3/etc/hadoop/
s3.enable.async=true
s3.use.async.client=true
s3.connection.timeout.sec=3600
s3.connection.acquisition.timeout.sec=3600
s3.client.service.threads=40
s3.max.request.concurrency=1000
s3.max.pending.requests=100000
gcs.enable.async=true
localfs.block.size=4096
localfs.enable.direct.io=false
# if mmap is enabled, direct io will be ignored
localfs.enable.mmap=false
localfs.enable.async.io=true
localfs.async.lib=iouring
localfs.reader.threads=40
minio.region=eu-central-2
minio.endpoint=http://minio-host-dummy:9000
minio.access.key=minio-access-key-dummy
minio.secret.key=minio-secret-key-dummy
redis.endpoint=localhost:6379
redis.access.key=redis-user-dummy
redis.secret.key=redis-password-dummy
# the number of messages that the receiver tries to poll each time, it should be >= 1 and <= 10
s3qs.poll.batch.size=3

###### query engine settings ######

### dynamic spitting ###
# split size will be set to this fixed value if it is positive
fixed.split.size=-1
# true to enable just-in-time splitting in ordered path
multi.split.for.ordered=true
# the size in bytes of a table scan split, 16MB by default
split.size.mb=64
# the number of rows in a table scan split, 2560000 by default, <= 0 for unlimited
split.size.rows=5120000
# the type of split size index to be used, can be cost_based or inverted
# before using cost_based, ensure data statistics are collected
splits.index.type=inverted
projection.read.enabled=false

### Presto/Trino connectors ###
record.cursor.enabled=false

### pixels-turbo - query scheduling ###
scaling.enabled=false
scaling.machine.service=general
scaling.mpp.queue.capacity=3
scaling.cf.queue.capacity=5
cloud.watch.metrics.namespace=Pixels
cloud.watch.metrics.dimension.name=cluster
cloud.watch.metrics.dimension.value=01
query.concurrency.metrics.name=query-concurrency
query.concurrency.report.period.sec=5

### pixels-turbo - query planning ###
join.large.side.completion.ratio=0.1
# the maximum size in megabytes of a broadcast table
join.broadcast.threshold.mb=256
# the maximum number of rows in a broadcast table
join.broadcast.threshold.rows=20480000
# the maximum (average) size of a partition in partitioned join
join.partition.size.mb=512
# the maximum (average) number of rows in a partition in partitioned join
join.partition.size.rows=20480000
# the maximum number of rows in a partition in aggregation
aggr.partition.size.rows=1280000

### pixels-turbo - query execution ###
executor.input.storage.scheme=s3
executor.intermediate.storage.scheme=s3
executor.intermediate.folder=/pixels-turbo/intermediate/
executor.output.storage.scheme=output-storage-scheme-dummy
executor.output.folder=output-folder-dummy
executor.stage.completion.ratio=0.6
executor.selectivity.enabled=true
# the number of threads used in each worker
executor.intra.worker.parallelism=8
# which cloud function service to use, can be lambda (AWS Lambda) , vhive (vHive) or spike
executor.function.service=lambda
# which method will be used for data exchanging, batch or stream
executor.exchange.method=batch
# the first port used by pipeline worker
executor.worker.exchange.port=50010
executor.worker.lease.period.ms=10000
executor.ordered.layout.enabled=false
executor.compact.layout.enabled=true
# the names of the cloud function workers
scan.worker.name=ScanWorker
partition.worker.name=PartitionWorker
broadcast.join.worker.name=BroadcastJoinWorker
broadcast.chain.join.worker.name=BroadcastChainJoinWorker
partitioned.join.worker.name=PartitionedJoinWorker
partitioned.chain.join.worker.name=PartitionedChainJoinWorker
sort.worker.name=SortWorker
sorted.join.worker.name=SortedJoinWorker
aggregation.worker.name=AggregationWorker

# the names of the spike cloud function workers
#scan.worker.name=pixels-worker-spike
#partition.worker.name=pixels-worker-spike
#broadcast.join.worker.name=pixels-worker-spike
#broadcast.chain.join.worker.name=pixels-worker-spike
#partitioned.join.worker.name=pixels-worker-spike
#partitioned.chain.join.worker.name=pixels-worker-spike
#aggregation.worker.name=pixels-worker-spike

# parameter for spike client
spike.cpu=8192
spike.memory=32768
spike.timeout=20
spike.hostname=127.0.0.1
spike.port=13306

# parameter for vhive client
vhive.hostname=localhost
vhive.port=50051

### pixels-scaling ###
scaling.metrics.server.port=18896
scaling.metrics.server.host=localhost
scaling.metrics.name=QueryConcurrencyMetrics

vm.auto.scaling.enabled=false
## choose auto scaling policy
vm.auto.scaling.policy=basic
# AMI id of pixels worker
vm.ami.id=ami-xxxxxxxxxx
# the name of the key pair for the instance
vm.key.name=pixels
# use `,` to add multiple security groups, e.g. `pixels-sg1,pixels-sg2`
vm.security.groups=pixels-sg1

# choose the suit python path
# if you use global python instead of python-venv, you can easily modify it to python
python.env.path = /home/ubuntu/dev/venvpixels/bin/python
# path of code used to forecast the resource usage
forecast.code.path = /home/ubuntu/dev/pixels/pixels-daemon/src/main/java/io/pixelsdb/pixels/daemon/scaling/policy/helper/forecast.py
# path of history data
pixels.history.data.dir=/home/ubuntu/opt/pixels/history/
# split cputime (ms)
cpuspl = [10000,60000,300000,600000]
# split mem (G)
memspl = [1,8,16,32,64]

### pixels-retina - writer buffer flush configuration ###

# number of rows recorded in memTable, must be a multiple of 64
retina.buffer.memTable.size=10240
# the scheme of the storage for retina buffer object storage, e.g., s3,minio
retina.buffer.object.storage.schema=s3
# the folder path for retina buffer object storage
retina.buffer.object.storage.folder=s3://home-dongyang/retina/pixels/
# default encoding level is EL2
retina.buffer.flush.encodingLevel=2
# default no nulls padding
retina.buffer.flush.nullsPadding=false
# threshold number of memTable to be dumped to file
retina.buffer.flush.count=20
# interval in seconds to flush into file
retina.buffer.flush.interval=30

### pixels-sink ###
sink.server.enabled=false

### pixels-index ###
index.server.enabled=false
enabled.single.point.index.schemes=rocksdb,rockset
enabled.main.index.scheme=sqlite
# s3 bucket for rockset (rocksdb-cloud)
index.rockset.s3.bucket=pixels-turbo-public
index.rockset.s3.prefix=test/rocksdb-cloud/
# local path for rockset (rocksdb-cloud)
index.rockset.local.data.path=/tmp/rocksdb_cloud_test
index.rockset.persistent.cache.path=/tmp/cache
# cache size for rockset (rocksdb-cloud)
index.rockset.persistent.cache.size.gb=1
index.rockset.read.only=false
# rocksdb data path
index.rocksdb.data.path=/tmp/rocksdb
# rocksdb write buffer size (default to 64MB)
index.rocksdb.write.buffer.size=67108864
# Whether to enable the latest version cache for SinglePointIndex
index.cache.enabled=false
# The maximum number of entries in the cache
index.cache.capacity=10000000
# The expiration time (in seconds) of cache entries
index.cache.expiration.seconds=3600
#  whether each index corresponds to its own column family
index.rocksdb.multicf=false
# the directory where the sqlite files of main index are stored, each main index is stored as a sqlite file
index.sqlite.path=/tmp/sqlite
index.server.host=localhost
index.server.port=18895
###### experimental settings ######
# the rate of free memory in jvm
experimental.gc.threshold=0.3