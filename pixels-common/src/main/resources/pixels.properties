###### pixels-daemon settings ######
# pixels.var.dir is where the lock files are created
pixels.var.dir=/home/pixels/opt/pixels/var/
# metadata database connection properties
metadata.db.driver=com.mysql.jdbc.Driver
metadata.db.user=pixels
metadata.db.password=password
metadata.db.url=jdbc:mysql://localhost:3306/pixels_metadata?useUnicode=true&characterEncoding=UTF-8&zeroDateTimeBehavior=convertToNull
# metadata server host and port
metadata.server.port=18888
metadata.server.host=localhost
# transaction server host and port
trans.server.port=18889
trans.server.host=localhost
# etcd connection properties
# multiple host can be listed as etcd.hosts=host0,host1,host2
etcd.hosts=localhost
etcd.port=2379
# metrics server properties
metrics.server.enabled=false
metrics.bytesms.interval=51200
metrics.reader.json.dir=/dev/shm/
metrics.node.text.dir=/home/pixels/opt/node_exporter/text/
metrics.reader.collect.prob=0.1

###### pixels-sink settings ######
# Presto/Trino connection properties
presto.pixels.jdbc.url=jdbc:trino://localhost:8080/pixels/tpch
presto.orc.jdbc.url=jdbc:trino://localhost:8080/hive/tpch
presto.user=pixels
presto.password=null
presto.ssl=false
presto.query.url=http://localhost:8080/v1/query

###### pixels-cache settings ######
cache.location=/mnt/ramfs/pixels.cache
cache.size=102400000
index.location=/mnt/ramfs/pixels.index
index.size=102400000
cache.storage.scheme=hdfs
cache.schema=pixels
cache.table=test_105
# lease ttl must be larger than heartbeat period
lease.ttl.seconds=20
# heartbeat period must be larger than 0
heartbeat.period.seconds=10
# set to false if storage.scheme is S3
enable.absolute.balancer=false
cache.enabled=false
cache.read.direct=false

###### storage engine settings ######

### pixels reader and writer settings ###
# properties for pixels writer
pixel.stride=10000
row.group.size=268435456
block.size=2147483648
block.replication=1
block.padding=true
encoding=true
compression.block.size=1
# row batch size for pixels record reader, default value is 10000
row.batch.size=10000

### file storage and I/O ###
# the scheme of the storage systems that are enabled, e.g., hdfs,file,s3,gcs,minio,redis
enabled.storage.schemes=s3,minio,file,hdfs
# which scheduler to use for read requests, valid values: noop, sortmerge, ratelimited
read.request.scheduler=sortmerge
read.request.merge.gap=0
# rate limits only work for s3+ratelimited
read.request.rate.limit.rps=16000
read.request.rate.limit.mbps=1200
read.request.enable.retry=true
read.request.max.retry.num=3
# the interval in milliseconds of retry queue checks
read.request.retry.interval.ms=1000
# the dir containing core-site.xml and hdfs-site.xml
hdfs.config.dir=/opt/hadoop-2.7.3/etc/hadoop/
s3.enable.async=true
s3.use.async.client=true
s3.connection.timeout.sec=3600
s3.connection.acquisition.timeout.sec=3600
s3.client.service.threads=40
s3.max.request.concurrency=1000
s3.max.pending.requests=100000
gcs.enable.async=true
localfs.block.size=4096
localfs.enable.direct.io=false
# if mmap is enabled, direct io will be ignored
localfs.enable.mmap=false
localfs.enable.async.io=false
localfs.reader.threads=40

###### query engine settings ######

### dynamic spitting ###
# split size will be set to this fixed value if it is positive
fixed.split.size=-1
# true to enable just-in-time splitting in ordered path
multi.split.for.ordered=true
# the size in bytes of a table scan split, 16MB by default
split.size.mb=64
# the number of rows in a table scan split, 2560000 by default, <= 0 for unlimited
split.size.rows=5120000
# the type of split size index to be used, can be cost_based or inverted
# before using cost_based, ensure data statistics are collected
splits.index.type=inverted
projection.read.enabled=false

### Presto/Trino connectors ###
record.cursor.enabled=false

### pixels-turbo - query scheduling ###
cloud.watch.metrics.namespace=Pixels
cloud.watch.metrics.dimension.name=cluster
cloud.watch.metrics.dimension.value=01
query.concurrency.metrics.name=query-concurrency
query.concurrency.report.period.sec=5
scaling.machine.service=ec2
scaling.cluster.queue.capacity=3
scaling.serverless.queue.capacity=5

### pixels-turbo - query planning ###
join.large.side.completion.ratio=0.1
# the maximum size in megabytes of a broadcast table
join.broadcast.threshold.mb=256
# the maximum number of rows in a broadcast table
join.broadcast.threshold.rows=20480000
# the maximum (average) size of a partition in partitioned join
join.partition.size.mb=512
# the maximum (average) number of rows in a partition in partitioned join
join.partition.size.rows=20480000
# the maximum number of rows in a partition in aggregation
aggr.partition.size.rows=1280000

### pixels-turbo - query execution ###
executor.input.storage.scheme=s3
executor.input.storage.endpoint=input-endpoint-dummy
executor.input.storage.access.key=input-ak-dummy
executor.input.storage.secret.key=input-sk-dummy
executor.intermediate.storage.scheme=s3
executor.intermediate.storage.endpoint=intermediate-endpoint-dummy
executor.intermediate.storage.access.key=intermediate-ak-dummy
executor.intermediate.storage.secret.key=intermediate-sk-dummy
executor.intermediate.folder=/pixels-lambda-test/
executor.output.storage.scheme=s3
executor.output.storage.endpoint=output-endpoint-dummy
executor.output.storage.access.key=output-ak-dummy
executor.output.storage.secret.key=output-sk-dummy
executor.output.folder=/pixels-lambda-test/
executor.stage.completion.ratio=0.6
executor.selectivity.enabled=true
# the number of threads used in each worker
executor.intra.worker.parallelism=8
# which cloud function service to use, can be lambda (AWS Lambda).
executor.function.service=lambda
# the names of the cloud function workers
scan.worker.name=ScanWorker
partition.worker.name=PartitionWorker
broadcast.join.worker.name=BroadcastJoinWorker
broadcast.chain.join.worker.name=BroadcastChainJoinWorker
partitioned.join.worker.name=PartitionedJoinWorker
partitioned.chain.join.worker.name=PartitionedChainJoinWorker
aggregation.worker.name=AggregationWorker

###### experimental settings ######
# the rate of free memory in jvm
experimental.gc.threshold=0.3